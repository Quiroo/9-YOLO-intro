{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "09-lightGBM-hyp-search-COLAB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quiroo/9-YOLO-intro/blob/master/09_lightGBM_hyp_search_COLAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwEZhc797Uma"
      },
      "source": [
        "# Hyper Opt Notebook\n",
        "Esta notebook busca hyper parametros del modelo Light GBM, genera un .csv con los valores de la iteraciones. Esto se hace para no solo probar la mejor (posible overfitting) sino otras configuraciones que arrojen accuracies similares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpPQtt5o9sQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c9ab0c-fbdd-466e-84d7-aee764f3871b"
      },
      "source": [
        "pip install pandas_summary"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas_summary\n",
            "  Downloading pandas_summary-0.0.7-py2.py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pandas_summary) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pandas_summary) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas_summary) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pandas_summary) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pandas_summary) (1.15.0)\n",
            "Installing collected packages: pandas-summary\n",
            "Successfully installed pandas-summary-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOY-pl6C7Umf"
      },
      "source": [
        "## Import modulos necesarios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXrjYcKl7Umg"
      },
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from pandas_summary import DataFrameSummary\n",
        "from lightgbm import LGBMRegressor"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLVyUZNu7Umh"
      },
      "source": [
        "## Load data procesada utilizando las notebooks entregadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpcR74z47Umh"
      },
      "source": [
        "df = pd.read_feather('train_normalized_data.fth')\n",
        "df_test = pd.read_feather('test_normalized_data.fth')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvL9-rCk7Umi"
      },
      "source": [
        "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen', 'Promo2Weeks', \n",
        "            'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'State', \n",
        "            'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_bool_fw', 'StateHoliday_bool_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
        "contin_vars = ['CompetitionDistance', \n",
        "               'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', 'Precipitationmm',\n",
        "               'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
        "               'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
        "               'AfterStateHoliday_bool', 'BeforeStateHoliday_bool', 'Promo', 'SchoolHoliday', 'StateHoliday_bool']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xLK-tBN7Umj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59b160b-df68-4251-aa65-4fcbba400c4f"
      },
      "source": [
        "# Split data into train/val and define X and y variables\n",
        "df_train = df[df.Date < datetime.datetime(2015, 7, 1)]  \n",
        "df_val = df[df.Date >= datetime.datetime(2015, 7, 1)]\n",
        "print(f'Cantidad en val: {len(df_val)}, porcentaje: {len(df_train)/(len(df_train) + len(df_val))}')\n",
        "\n",
        "y_out_columns = ['Sales']\n",
        "X_train = df_train[cat_vars + contin_vars]\n",
        "X_val = df_val[cat_vars + contin_vars]\n",
        "X_test = df_test[cat_vars + contin_vars]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad en val: 30188, porcentaje: 0.9642465458145908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KZ4cr9i7Umk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fecb75a5-9898-4a7a-b0b5-c32f884ad2a4"
      },
      "source": [
        "X_train.shape, X_val.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((814150, 40), (30188, 40))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCB6amIf7Uml"
      },
      "source": [
        "# Normalize output and determine wether to use log_output \n",
        "log_output = True\n",
        "    \n",
        "if log_output:\n",
        "    # Escala logaritmica\n",
        "    max_log_y = np.max(np.log(df[y_out_columns])).values\n",
        "    y_train = np.log(df_train[y_out_columns].values)/max_log_y\n",
        "    y_val = np.log(df_val[y_out_columns].values)/max_log_y\n",
        "else:\n",
        "    # Normalización\n",
        "    y_mean = df_train[y_out_columns].mean().values\n",
        "    y_std = df_train[y_out_columns].std().values\n",
        "    y_train = (df_train[y_out_columns].values - y_mean)/y_std\n",
        "    y_val = (df_val[y_out_columns].values - y_mean)/y_std"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBUV3bj77Umm"
      },
      "source": [
        "## Hyper Opt Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bjh8JKf7Umn"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from hyperopt import hp, tpe\n",
        "from hyperopt.fmin import fmin\n",
        "from hyperopt import Trials"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcJRJ2PF7Umn"
      },
      "source": [
        "# Definimos una función que nos devuelva los y_pred desnormalizados (lo vamos a usar para calcular el score a optimizar)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRyXWBrW7Umn"
      },
      "source": [
        "def getValPred(model):\n",
        "    if log_output:\n",
        "        y_pred = np.exp(model.predict(X_val, verbose=1)*max_log_y)\n",
        "    else:\n",
        "        y_pred = model.predict(X_val, verbose=1)*y_std + y_mean\n",
        "    return y_pred"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pob3TWIf7Umo"
      },
      "source": [
        "# Normalize output and determine wether to use log_output \n",
        "log_output = True\n",
        "    \n",
        "if log_output:\n",
        "    # Escala logaritmica\n",
        "    max_log_y = np.max(np.log(df[y_out_columns])).values\n",
        "    y_train = np.log(df_train[y_out_columns].values)/max_log_y\n",
        "    y_val = np.log(df_val[y_out_columns].values)/max_log_y\n",
        "else:\n",
        "    # Normalización\n",
        "    y_mean = df_train[y_out_columns].mean().values\n",
        "    y_std = df_train[y_out_columns].std().values\n",
        "    y_train = (df_train[y_out_columns].values - y_mean)/y_std\n",
        "    y_val = (df_val[y_out_columns].values - y_mean)/y_std"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqYmu4sm7Umo"
      },
      "source": [
        "# Definimos la función objetivo\n",
        "def objective(params):\n",
        "    params = {\n",
        "        'reg_lambda': int(params['reg_lambda']),\n",
        "        'num_leaves': int(params['num_leaves']),\n",
        "        'max_depth': int(params['max_depth']),\n",
        "        'n_estimators': int(params['n_estimators']),\n",
        "        'learning_rate': '{:.4f}'.format(params['learning_rate'])\n",
        "    }\n",
        "    \n",
        "    # Fixed Params\n",
        "    min_child_samples=5\n",
        "    max_depth = 500\n",
        "    min_child_samples= 200 \n",
        "    reg_alpha=1.0\n",
        "    colsample_bytree=0.519264\n",
        "    min_child_weight=0.0\n",
        "    \n",
        "    clf = LGBMRegressor(min_child_samples=min_child_samples, **params,\n",
        "                        reg_alpha=reg_alpha, colsample_bytree=colsample_bytree, min_child_weight=min_child_weight,n_jobs=8)\n",
        "    fit_params={\"early_stopping_rounds\":100, \n",
        "            \"eval_metric\" : 'l2', \n",
        "            \"eval_set\" : [(X_val, y_val.reshape(-1))],\n",
        "            'eval_names': ['valid'],\n",
        "            'verbose': 0,\n",
        "            'feature_name': 'auto', # that's actually the default\n",
        "            'categorical_feature': cat_vars\n",
        "           }\n",
        "    clf.fit(X_train, y_train.reshape(-1), **fit_params)\n",
        "    \n",
        "    y_pred = getValPred(clf)\n",
        "    score = -1*(np.sqrt((((df_val['Sales'].values - y_pred)/df_val['Sales'].values)**2).sum()/len(y_pred))) # realizo el NEGATIVE RMSE\n",
        "    return -score\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCj_ljIV7Ump",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5020e24-7b24-497f-ab5e-c1173899efbe"
      },
      "source": [
        "space = {'max_depth': hp.quniform('max_depth',400,600,20),\n",
        "         'reg_lambda': hp.quniform('reg_lambda',0,40,1),\n",
        "         'num_leaves': hp.quniform('num_leaves',50,80,5),\n",
        "         'n_estimators': hp.quniform('n_estimators',1000,1200,50),\n",
        "         'learning_rate': hp.loguniform('learning_rate', -4, -2)\n",
        "}\n",
        "\n",
        "tpe_trials = Trials()\n",
        "\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            trials = tpe_trials,\n",
        "            verbose=2,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s, best loss: ?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
            "New categorical_feature is ['Assortment', 'CompetitionMonthsOpen', 'CompetitionOpenSinceYear', 'Day', 'DayOfWeek', 'Events', 'Month', 'Promo2SinceYear', 'Promo2Weeks', 'PromoInterval', 'Promo_bw', 'Promo_fw', 'SchoolHoliday_bw', 'SchoolHoliday_fw', 'State', 'StateHoliday', 'StateHoliday_bool_bw', 'StateHoliday_bool_fw', 'Store', 'StoreType', 'Week', 'Year']\n",
            "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 100/100 [4:36:25<00:00, 165.86s/it, best loss: 0.11296633050265104]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3FD7jfF7Ump",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c123f7-d4c1-4d51-d86a-ebd085234f12"
      },
      "source": [
        "best"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.050612479556637426,\n",
              " 'max_depth': 460.0,\n",
              " 'n_estimators': 1150.0,\n",
              " 'num_leaves': 55.0,\n",
              " 'reg_lambda': 17.0}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyjwIi9V7Ump"
      },
      "source": [
        "## Guardamos las iteraciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE7Oykjh7Ump"
      },
      "source": [
        "tpe_results = pd.DataFrame({'loss': [x['loss'] for x in tpe_trials.results], \n",
        "                            'iteration': tpe_trials.idxs_vals[0]['max_depth'],\n",
        "                            'max_depth': tpe_trials.idxs_vals[1]['max_depth']})\n",
        "tpe_results.set_index('iteration')\n",
        "tpe_results.loc[tpe_trials.idxs_vals[0]['learning_rate'], 'learning_rate']  = tpe_trials.idxs_vals[1]['learning_rate']\n",
        "tpe_results.loc[tpe_trials.idxs_vals[0]['reg_lambda'], 'reg_lambda']  = tpe_trials.idxs_vals[1]['reg_lambda']\n",
        "tpe_results.loc[tpe_trials.idxs_vals[0]['num_leaves'],  'num_leaves']  = tpe_trials.idxs_vals[1][ 'num_leaves']\n",
        "tpe_results.loc[tpe_trials.idxs_vals[0]['n_estimators'], 'n_estimators']  = tpe_trials.idxs_vals[1]['n_estimators']\n",
        "\n",
        "tpe_results.to_csv(f'09-lightGBM-hyp-search_iterations.csv')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZaGswQAAxoR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "outputId": "42f05fd3-a257-414b-a114-ef3c373b6e31"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('09-lightGBM-hyp-search_iterations.csv')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a86302fd-83c7-4b1d-b09a-762d8dbc3d3b\", \"09-lightGBM-hyp-search_iterations.csv\", 6934)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbj-CuhH7VuF"
      },
      "source": [
        "Continua en la siguiente notebook 9.1-lightGBM-hyp-search-COLAB\n"
      ]
    }
  ]
}